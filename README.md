# GLADE-Net 
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/pytorch-1.9+-orange.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

learning framework for perceptual dehazing of high-resolution remote sensing images: Phase 1 (Global Attention Stage): Uses a UFormer-based encoderâ€“decoder with grid patching to capture global atmospheric and ecological context. Phase 2 (Laplacian-Guided GAN Stage): Refines outputs via a dual-branch GAN that operates in both RGB and Laplacian domains, enhancing edges, textures, and perceptual realism.
GLADE-Net achieves state-of-the-art PSNR and LPIPS on SateHaze1k and RICE datasets, balancing structural fidelity and visual quality for environmental and ecological monitoring applications.

---

## ğŸ§  Overview

### **Phase 1 â€” Global Attention Dehazing**
This stage focuses on **coarse haze removal** and global feature restoration using a UFormer-based encoderâ€“decoder optimized for remote sensing imagery.  

**Key Features:**
- ğŸŒ **Global Attention Encoderâ€“Decoder (UFormer):** Captures large-scale atmospheric and ecological dependencies.  
- âš¡ **PyTorch Lightning Framework:** Streamlined training loop with mixed-precision (FP16) support.  
- ğŸ§© **Grid-Based Dataset Loader:** Performs on-the-fly tiling, augmentation, and efficient handling of high-resolution inputs.  
- ğŸ“ˆ **Gradual Frequency Loss:** Charbonnier-based loss with PSNR/SSIM evaluation metrics for stable optimization.  
- ğŸ›°ï¸ **Inference Tools:** Enable large-scale aerial image restoration through automated grid merging and tiling scripts.  

---

### **Phase 2 â€” Laplacian-Guided Perceptual Refinement**
This stage enhances **visual quality and perceptual realism** of the dehazed outputs generated by Phase 1.  
It integrates adversarial learning and frequency-domain guidance for fine detail enhancement.  

**Key Features:**
- ğŸ¨ **Dual-Branch GAN:** Operates jointly in RGB and Laplacian domains to restore both structural and textural details.  
- ğŸ” **Laplacian Pyramid Guidance:** Multi-scale refinement of edges and textures using Laplacian-based supervision.  
- ğŸ‘ï¸ **Perceptual and Adversarial Losses:** Improve realism and visual fidelity while maintaining quantitative accuracy.  
- ğŸ”¬ **High-Resolution Enhancement:** Designed for ecological monitoring and environmental analysis applications.  


---

## âš™ï¸ Installation
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```
> Tested with Python 3.8+ and PyTorch â‰¥ 2.0.

---

## ğŸ§© Dataset Layout

Example folder structure:

/path/to/RICE_DATASET/
  â”œâ”€â”€ train/
  â”‚   â”œâ”€â”€ cloud/   # hazy or cloudy input images
  â”‚   â””â”€â”€ label/   # clean ground-truth targets
  â””â”€â”€ test/
      â”œâ”€â”€ cloud/
      â””â”€â”€ label/

---

## ğŸ“Š Datasets

| No. | Dataset | Year | Pub. | Number | Image Size | Type | Download |
|:---:|:--------|:----:|:----:|:-------:|:-----------:|:------:|:----------:|
| 01 | [**SateHaze1k**](https://ieeexplore.ieee.org/document/9093471) | 2017 | WACV | 400Ã—3 | 512Ã—512 | Synthetic | [ğŸ”— Download](https://www.dropbox.com/scl/fi/wtga5ltw5vby5x7trnp0p/Haze1k.zip?rlkey=70s52w3flhtif020nx250jru3&e=1&dl=0) |
| 02 | [**RICE (RICE1 & RICE2)**](https://arxiv.org/abs/1901.00600) | 2019 | arXiv | 950 | 512Ã—512 | Synthetic | [ğŸ”— Download](https://github.com/BUPTLdy/RICE_DATASET) |

**Notes:**
- *SateHaze1k* is divided into **Thin**, **Moderate**, and **Thick** subsets simulating different haze densities.  
- *RICE1* and *RICE2* include paired cloudy and clear-sky remote sensing images for evaluating dehazing and cloud removal performance.
---

## ğŸ§± Step 1: Grid-Based Dataset Preparation

To generate 128Ã—128 tiles for training and validation:

```bash
python GLADE-Net/tools/grid_crop_pairs.py \
  --root    dataset \ #path to your dataset train[cloud, label], test[cloud, label]
  --out_root dataset_grid \
  --tile_w 128 --tile_h 128 --overlap 0

```
Output tiles will be created under:
dataset_grid/train/
dataset_grid/test/

---

## ğŸš€ Step 2: Training Phase 1 

Run training from the project root:
```bash
cd GLADE-Net
python -m src.train \
  --train_dir dataset_grid/train \
  --val_dir   dataset_grid/test \
  --img_size 128 --batch_size 1 --epochs 2500 --devices 1 \
  --precision 16 --accumulate 2 --lr 2e-4 \
  --save_ckpt Phase_1.ckpt
```
You can resume training with:

python -m src.train --train_dir ... --val_dir ... --resume /path/to/checkpoint.ckpt

---

## ğŸ” Step 3: Inference Phase 1 (Grid UFormer)

Run inference on cloudy test images using the trained checkpoint:
```bash
python GLADE-Net/tools/inference_grid.py \
  --weights Phase_1.ckpt \
  --input   dataset/test/cloud \
  --output  dataset/test/output_phase_1 \
  --tile 512

```

The script will:
- Load each image, pad to a multiple of the tile size.
- Partition into non-overlapping grids.
- Apply the UFormer model patch-by-patch.
- Reconstruct and crop back to the original size.
- Save restored outputs under output_phase_1/.

Example output folders:
dataset/test/cloud/           # input hazy images
dataset/test/label/           # ground truth
dataset/test/output_phase_1/  # Phase 1 restored results

---

## ğŸ§® Step 4: Training Phase 2 â€“ Perceptual GAN Refinement 

The next phase will use the output from Phase 1 (restored dehazed images) paired with their ground-truth targets to train a perceptual enhancement GAN.  
This stage focuses on:
- Fine texture recovery and color correction.
- Multi-domain learning (RGB + Laplacian).
- Perceptual and adversarial loss integration.

```bash
python GLADE-Net/Phase_2/code/train.py\
     -opt GLADE-Net/Phase_2/code/options/train/train_second_phase.json
```
---
## ğŸ” Step 5: Inference Phase 2

Run inference on cloudy test images using the trained weight:
```bash
python GLADE-Net/Phase_2/code/test.py\
     -opt GLADE-Net/Phase_2/code/options/test/test_second_phase.json 
```
---
### ğŸš€ Quick Demo â€“ Try GLADE-Net in One Notebook

If you want to quickly test **GLADE-Net (Phase 1 + Phase 2)** without setting up the entire training pipeline, we provide an interactive demo notebook.  
You can open and run it directly on GitHub or in Google Colab.

**Demo Notebook:**  
[![train_inference_Phase_1_and_2_demo_tutorial.ipynb](https://img.shields.io/badge/Open%20Demo%20Notebook-Click%20Here-blue?style=for-the-badge)](https://github.com/AnasHXH/GLADE-Net/blob/main/notebooks/train_inference_Phase_1_and_2_demo_tutorial.ipynb)

ğŸ“˜ This notebook demonstrates:
- Loading pretrained Phase 1 and Phase 2 models  
- Running inference on sample remote-sensing images  
- Visualizing haze removal and perceptual enhancement results  

---

## ğŸ§  Citation

If you use this repository, please cite the paper:
Common soon


---

