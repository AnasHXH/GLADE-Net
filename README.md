# GLADE-Net 
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/pytorch-1.9+-orange.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

learning framework for perceptual dehazing of high-resolution remote sensing images: Phase 1 (Global Attention Stage): Uses a UFormer-based encoderâ€“decoder with grid patching to capture global atmospheric and ecological context. Phase 2 (Laplacian-Guided GAN Stage): Refines outputs via a dual-branch GAN that operates in both RGB and Laplacian domains, enhancing edges, textures, and perceptual realism.
GLADE-Net achieves state-of-the-art PSNR and LPIPS on SateHaze1k and RICE datasets, balancing structural fidelity and visual quality for environmental and ecological monitoring applications.

---

## ğŸ§  Overview

### **Phase 1 â€” Global Attention Dehazing**
This stage focuses on **coarse haze removal** and global feature restoration using a UFormer-based encoderâ€“decoder optimized for remote sensing imagery.  

**Key Features:**
- ğŸŒ **Global Attention Encoderâ€“Decoder (UFormer):** Captures large-scale atmospheric and ecological dependencies.  
- âš¡ **PyTorch Lightning Framework:** Streamlined training loop with mixed-precision (FP16) support.  
- ğŸ§© **Grid-Based Dataset Loader:** Performs on-the-fly tiling, augmentation, and efficient handling of high-resolution inputs.  
- ğŸ“ˆ **Gradual Frequency Loss:** Charbonnier-based loss with PSNR/SSIM evaluation metrics for stable optimization.  
- ğŸ›°ï¸ **Inference Tools:** Enable large-scale aerial image restoration through automated grid merging and tiling scripts.  

---

### **Phase 2 â€” Laplacian-Guided Perceptual Refinement**
This stage enhances **visual quality and perceptual realism** of the dehazed outputs generated by Phase 1.  
It integrates adversarial learning and frequency-domain guidance for fine detail enhancement.  

**Key Features:**
- ğŸ¨ **Dual-Branch GAN:** Operates jointly in RGB and Laplacian domains to restore both structural and textural details.  
- ğŸ” **Laplacian Pyramid Guidance:** Multi-scale refinement of edges and textures using Laplacian-based supervision.  
- ğŸ‘ï¸ **Perceptual and Adversarial Losses:** Improve realism and visual fidelity while maintaining quantitative accuracy.  
- ğŸ”¬ **High-Resolution Enhancement:** Designed for ecological monitoring and environmental analysis applications.  


---

## ğŸ“‚ Repository Structure

GLADE-Net-Phase1/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ gladenet_phase1/
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â””â”€â”€ uformer_module.py     # UFormer architecture
â”‚   â””â”€â”€ train.py                      # PyTorch-Lightning training entry
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ grid_crop_pairs.py            # Pre-tile large training images
â”‚   â”œâ”€â”€ grid_crop.py                  # Simple grid crop utility
â”‚   â””â”€â”€ inference_grid.py             # Phase-1 inference (new version)
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ train/
â”‚   â””â”€â”€ test/
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## âš™ï¸ Installation

python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

> Tested with Python 3.8+ and PyTorch â‰¥ 2.0.

---

## ğŸ§© Dataset Layout

Example folder structure:

/path/to/RICE_DATASET/
  â”œâ”€â”€ train/
  â”‚   â”œâ”€â”€ cloud/   # hazy or cloudy input images
  â”‚   â””â”€â”€ label/   # clean ground-truth targets
  â””â”€â”€ test/
      â”œâ”€â”€ cloud/
      â””â”€â”€ label/

---

## ğŸ§± Step 1: Grid-Based Dataset Preparation

To generate 128Ã—128 tiles for training and validation:

python tools/grid_crop_pairs.py \
  --root /media/anas/Data/Anas_Work_2/Grid_SPSR_paper/Grid_spsr_code/dataset \
  --out_root /media/anas/Data/Anas_Work_2/Grid_SPSR_paper/Grid_spsr_code/dataset_grid \
  --tile_w 128 --tile_h 128 --overlap 0

Output tiles will be created under:
dataset_grid/train/
dataset_grid/test/

---

## ğŸš€ Step 2: Training Phase 1 (UFormer)

Run training from the project root:

python -m src.train \
  --train_dir /media/anas/Data/Anas_Work_2/Grid_SPSR_paper/Grid_spsr_code/dataset_grid/train \
  --val_dir   /media/anas/Data/Anas_Work_2/Grid_SPSR_paper/Grid_spsr_code/dataset_grid/test \
  --img_size 128 \
  --batch_size 1 \
  --epochs 2500 \
  --devices 1 \
  --precision 16 \
  --accumulate 2 \
  --lr 2e-4 \
  --save_ckpt /media/anas/Data/Anas_Work_2/Grid_SPSR_paper/Grid_spsr_code/dataset_grid/Grid_RICE_1_new_dataset_final.ckpt

You can resume training with:

python -m src.train --train_dir ... --val_dir ... --resume /path/to/checkpoint.ckpt

---

## ğŸ” Step 3: Inference Phase 1 (Grid UFormer)

Run inference on cloudy test images using the trained checkpoint:

python tools/inference_grid.py \
  --weights /media/anas/Data/Anas_Work_2/Grid_SPSR_paper/Grid_spsr_code/Grid/Grid_haze1k_freq_loss_train_with_test_full.ckpt \
  --input /media/anas/Data/Anas_Work_2/Grid_SPSR_paper/Grid_spsr_code/dataset/test/cloud \
  --output /media/anas/Data/Anas_Work_2/Grid_SPSR_paper/Grid_spsr_code/dataset/test/output_phase_1 \
  --tile 512 \
  --device cuda

The script will:
- Load each image, pad to a multiple of the tile size.
- Partition into non-overlapping grids.
- Apply the UFormer model patch-by-patch.
- Reconstruct and crop back to the original size.
- Save restored outputs under output_phase_1/.

Example output folders:
dataset/test/cloud/           # input hazy images
dataset/test/label/           # ground truth
dataset/test/output_phase_1/  # Phase 1 restored results

---

## ğŸ§® Step 4: Phase 2 â€“ Perceptual GAN Refinement (Coming Soon)

The next phase will use the output from Phase 1 (restored dehazed images) paired with their ground-truth targets to train a perceptual enhancement GAN.  
This stage focuses on:
- Fine texture recovery and color correction.
- Multi-domain learning (RGB + Laplacian).
- Perceptual and adversarial loss integration.

### Placeholder for Phase 2 Code

#############################
# Phase 2 - Enhancement GAN
# (To be implemented)
#############################

Example usage (will be updated later):

python tools/inference_phase2.py \
  --weights /path/to/phase2_weights.ckpt \
  --input dataset/test/output_phase_1 \
  --gt dataset/test/label \
  --output dataset/test/output_phase_2

---

## ğŸ§  Citation

If you use this repository, please cite the paper:

Ali, A. M., Boulila, W., Benjdira, B., Ammar, A. et al.  
â€œCLEAR-Net: A Cascaded Local and External Attention Network for Enhanced Dehazing of Remote Sensing Images.â€  
IEEE JSTARS, 2025.

---

## ğŸ“œ License

MIT License  
Copyright (c) 2025 Anas M. Ali
